{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8ea00-7ecc-4ea7-8fee-2fae207a5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "import sklearn\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import ast\n",
    "import os\n",
    "from scipy import stats\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732e4d5-7b91-475b-ba3b-84646a593bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(deriv_data, shuffle=True, random_state=42,train_perc=0.85):\n",
    "    # Divide patients to train / validation / groups\n",
    "    \n",
    "    #random.seed(random_state)\n",
    "    # Divide patients to train / validation / groups\n",
    "    \n",
    "    patient_list = deriv_data['patient_id'].unique()\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.shuffle(patient_list)\n",
    "    \n",
    "    # Calculate the number of items in each sublist\n",
    "    total_items = len(patient_list)\n",
    "    train_size = int(total_items * train_perc)\n",
    "    val_size = total_items - train_size  # To ensure all items are included\n",
    "\n",
    "    # Divide the list into sublists\n",
    "    train_list = patient_list[:train_size]\n",
    "    val_list = patient_list[train_size:]\n",
    "    \n",
    "    train_data = deriv_data[deriv_data['patient_id'].isin(train_list)].reset_index(drop=True)\n",
    "    val_data = deriv_data[deriv_data['patient_id'].isin(val_list)].reset_index(drop=True)\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db07e90-19ef-47b6-851b-d62a253c6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_color = '#777777'\n",
    "AML_color = '#BF9F45'       #'#E24A33'\n",
    "MDS_color = '#348ABD'\n",
    "MF_color = '#2b6e2a'       #'#155236'\n",
    "any_MN_color = '#2d0e3d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05dd66c-a0ad-4195-bc55-d9142a587c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AML_cmap = LinearSegmentedColormap.from_list('MF_cmap', ['#FFFFFF', AML_color])\n",
    "MDS_cmap = LinearSegmentedColormap.from_list('MF_cmap', ['#FFFFFF', MDS_color])\n",
    "MF_cmap = LinearSegmentedColormap.from_list('MF_cmap', ['#FFFFFF', MF_color])\n",
    "any_MN_cmap = LinearSegmentedColormap.from_list('any_MN_cmap', ['#FFFFFF', any_MN_color])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48204b9-b6f5-496c-adcd-fedbcd26124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b375567-d01c-4c3b-a9f4-8787ca23254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_train_data(train_data, shuffle=True, random_state=42, ratio=100):\n",
    "    \n",
    "    ## Reduce number of healthy datapoints -- 100 healthy controls per patient\n",
    "    \n",
    "    train_disease = train_data[train_data['disease_status'] == 1]\n",
    "    train_healthy = train_data[train_data['disease_status'] == 0]\n",
    "    n_train_d = len(train_disease['patient_id'].unique())\n",
    "    n_train_h = n_train_d * ratio\n",
    "    healthy_list = train_healthy['patient_id'].unique()\n",
    "    \n",
    "    #random.seed(random_state)\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.shuffle(healthy_list)\n",
    "    \n",
    "    healthy_subset = healthy_list[:n_train_h]\n",
    "    train_healthy_subset = train_healthy[train_healthy['patient_id'].isin(healthy_subset)].reset_index(drop=True)\n",
    "    train_data = pd.concat([train_disease, train_healthy_subset], axis=0)\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba7e0a-1cd0-4840-a849-a7e6221a453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '~/mounts/research/husdatalake/disease/scripts/Preleukemia/oona_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195be4a-4eb4-4e2f-b1ca-9b8f9f532bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = 'MF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866dbde-9c00-400a-b3da-a5683d9b4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if disease == 'de_novo_AML':\n",
    "    cmap = AML_cmap\n",
    "if disease == 'MDS':\n",
    "    cmap = MDS_cmap\n",
    "if disease == 'MF':\n",
    "    cmap = MF_cmap\n",
    "if disease == 'any_MN':\n",
    "    cmap = any_MN_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91404d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "method = 'youden'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09a91f-4ce5-4bec-9108-38859eea32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting if patient will get disease during the next X days\n",
    "prediction_horizon = 365 *5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd250e-e5ae-408b-9648-c557dc882bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many previous datapoints are needed for applying trajectory model\n",
    "min_points=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c84f7-c45a-4ad7-9bab-b6f4eddb028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to include hard positives\n",
    "include_hp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14512659-8784-4760-97fe-eb433536302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read first stage cox model binary threshold\n",
    "import json\n",
    "\n",
    "with open('results/basic_model/' + disease + '_threshold_youden.json', 'r') as f:\n",
    "    thresholds = json.load(f)\n",
    "\n",
    "binary_threshold = thresholds['med']\n",
    "\n",
    "print(binary_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0378229-2356-4221-a165-5f78f73efbf9",
   "metadata": {},
   "source": [
    "# 1. Read risk score feature data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542e2bc-4e24-4403-8f01-f0a8611c6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "if include_hp == True:\n",
    "    features_df = pd.read_csv('trajectory_model/' + disease + '_full_risk_score_deriv_data_with_hp.csv', engine='c', low_memory=False)\n",
    "else:\n",
    "    features_df = pd.read_csv('trajectory_model/' + disease + '_full_risk_score_deriv_data.csv', engine='c', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc77190",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['score_gt_thresh'] = features_df['risk_score_now'] >= binary_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebff5f5-640b-47dd-b6bc-58c2652202a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label based on prediction horizon\n",
    "features_df['label'] = np.where(\n",
    "    (features_df['disease_status'] == 1) & (features_df['time_to_dg'] >= -prediction_horizon),\n",
    "    1,\n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a97f837-b9c7-4c63-94f2-32e93ff453ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disease patients to positive\n",
    "features_df.loc[features_df['label'] == 1, 'time_to_dg'] = features_df.loc[features_df['label'] == 1, 'time_to_dg'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def8c7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Use only rows with n_prev > min_points\n",
    "features_df = features_df[features_df['n_prev']>= min_points]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a9b692-b79b-4661-b330-fad2a5ede328",
   "metadata": {},
   "source": [
    "# Optimize hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcf2f71-76ed-4f2f-b8f8-5fadf68169d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'objective' : ['survival:cox'],\n",
    "    'eval_metric' : ['cox-nloglik'],\n",
    "    'max_depth': [3, 5, 6],\n",
    "    'learning_rate': [0.01, 0.05],\n",
    "    'subsample': [0.6, 1.0],\n",
    "    'colsample_bytree': [0.6, 1.0],\n",
    "    'reg_alpha': [0, 0.5],\n",
    "    'reg_lambda': [0.5, 2, 5],\n",
    "    'tree_method': ['hist']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b0fb39-8313-4073-8d0b-a630e0cafaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d011b9c8-a85d-4473-a1ed-4abcad5bdfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nrounds = 1000\n",
    "early_stop = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9195ef0-ccb3-45ab-89fc-fa78b4985b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_grid = ParameterGrid(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601f0848-f1b5-49ff-ba9b-a53e75b660c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parameters_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1871fd-3eb8-4c2e-ab43-d701c423ebc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['params', 'AUC_mean', 'AUC_std', 'AUCPR_mean', 'AUCPR_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69f0b9b3-4bd1-4c7b-8386-5ef8e259aec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "671adf7f-eefd-4a10-a8ff-06d230acfe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for params in parameters_grid:\n",
    "    \n",
    "    print(f'\\nTRAINING MODEL WITH PARAMETERS:\\n {params} \\n{cv}-FOLD CROSS VALIDATION')\n",
    "    \n",
    "    AUCs = []\n",
    "    AUCPRs = []\n",
    "    \n",
    "    for i in range(cv):\n",
    "    \n",
    "        rs = random.randint(1, 1000)\n",
    "        print(rs)\n",
    "        \n",
    "        train, val = train_val_split(features_df,random_state=rs, train_perc=0.85)\n",
    "        \n",
    "        # Sanity check - is any of test indices in validation or training sets\n",
    "        print('\\nSanity check: Is there any validaion data in train set')\n",
    "        train_ht = list(train['patient_id'].unique())\n",
    "        validation_ht = list(val['patient_id'].unique())\n",
    "        #test_ht = list(test_data['henkilotunnus'].unique())\n",
    "        val_in_train = np.intersect1d(validation_ht, train_ht).size > 0\n",
    "        print(val_in_train)\n",
    "        \n",
    "        # Train classifier on rows with enough prior data\n",
    "        train = train[train['n_prev'] >= min_points].copy()\n",
    "        \n",
    "        # <ratio> controls per 1 patient\n",
    "        print('N train data rows before reduction: ', len(train))\n",
    "        train = reduce_train_data(train, ratio=ratio)\n",
    "        print('N train data rows after reduction: ', len(train))\n",
    "        \n",
    "        X_train = train.drop(columns=['patient_id', 'disease_status', 'time_to_dg', 'label', 'score_gt_thresh'])\n",
    "        y_train = train['time_to_dg']\n",
    "        X_val = val.drop(columns=['patient_id', 'disease_status', 'time_to_dg', 'label', 'score_gt_thresh'])\n",
    "        y_val = val['time_to_dg']\n",
    "        \n",
    "        # Create DMatrix for XGBoost\n",
    "        dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "        dval = xgb.DMatrix(X_val, label=y_val)\n",
    "        \n",
    "        # Use validation set to watch performance\n",
    "        watchlist = [(dtrain,'train'), (dval,'eval')]\n",
    "        \n",
    "        # Store validation results\n",
    "        evals_results = {}\n",
    "        \n",
    "        # Train the model\n",
    "        xgb_model = xgb.train(params, dtrain, num_boost_round=nrounds, early_stopping_rounds=early_stop, evals=watchlist, evals_result=evals_results, verbose_eval=50)\n",
    "        \n",
    "        # Predict risk scores\n",
    "        risk_scores_train = xgb_model.predict(dtrain)\n",
    "        risk_scores_val = xgb_model.predict(dval)\n",
    "        \n",
    "        # Add risk scores to the dataframe\n",
    "        train['risk_score'] = risk_scores_train\n",
    "        val['risk_score'] = risk_scores_val\n",
    "    \n",
    "        # Predict risk scores\n",
    "        risk_scores_train = xgb_model.predict(dtrain)\n",
    "        risk_scores_val = xgb_model.predict(dval)\n",
    "        \n",
    "        # Add risk scores to the dataframe\n",
    "        train['risk_score'] = risk_scores_train\n",
    "        val['risk_score'] = risk_scores_val\n",
    "        \n",
    "        # Calculate C-index for validation set\n",
    "        # Negative times to positive for getting c-index\n",
    "        val['time_to_dg'] = val['time_to_dg'].apply(lambda x: -x if x < 0 else x)\n",
    "        \n",
    "        try:\n",
    "            # ROC-AUC\n",
    "            fpr, tpr, thresholds = roc_curve(val['label'], val['risk_score'])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            AUCs.append(roc_auc)\n",
    "\n",
    "            # AUCPR\n",
    "            precision, recall, _ = precision_recall_curve(val['label'], val['risk_score'])\n",
    "            aucpr = auc(recall, precision)\n",
    "            AUCPRs.append(aucpr)\n",
    "\n",
    "\n",
    "        except:\n",
    "            print('Something went wrong with model training with current parameters.')\n",
    "    \n",
    "    result_df.loc[len(result_df.index)] = [params, np.mean(AUCs), np.std(AUCs), np.mean(AUCPRs), np.std(AUCPRs)]\n",
    "    # Save results at each iteration\n",
    "    result_df.to_csv('trajectory_model/' + disease + '_hyperparameter_results_cv.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fa7ca2-6b85-48a9-942d-a1bacc326787",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
