{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e8ea00-7ecc-4ea7-8fee-2fae207a5f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "import sklearn\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.metrics import make_scorer, average_precision_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import ast\n",
    "import os\n",
    "from scipy import stats\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4732e4d5-7b91-475b-ba3b-84646a593bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(deriv_data, shuffle=True, random_state=42,train_perc=0.85):\n",
    "    # Divide patients to train / validation / groups\n",
    "    \n",
    "    #random.seed(random_state)\n",
    "    # Divide patients to train / validation / groups\n",
    "    \n",
    "    patient_list = deriv_data['patient_id'].unique()\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.shuffle(patient_list)\n",
    "    \n",
    "    # Calculate the number of items in each sublist\n",
    "    total_items = len(patient_list)\n",
    "    train_size = int(total_items * train_perc)\n",
    "    val_size = total_items - train_size  # To ensure all items are included\n",
    "\n",
    "    # Divide the list into sublists\n",
    "    train_list = patient_list[:train_size]\n",
    "    val_list = patient_list[train_size:]\n",
    "    \n",
    "    train_data = deriv_data[deriv_data['patient_id'].isin(train_list)].reset_index(drop=True)\n",
    "    val_data = deriv_data[deriv_data['patient_id'].isin(val_list)].reset_index(drop=True)\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db07e90-19ef-47b6-851b-d62a253c6955",
   "metadata": {},
   "outputs": [],
   "source": [
    "healthy_color = '#777777'\n",
    "AML_color = '#BF9F45'       #'#E24A33'\n",
    "MDS_color = '#348ABD'\n",
    "MF_color = '#2b6e2a'       #'#155236'\n",
    "any_MN_color = '#2d0e3d'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f05dd66c-a0ad-4195-bc55-d9142a587c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "AML_cmap = LinearSegmentedColormap.from_list('MF_cmap', ['#FFFFFF', AML_color])\n",
    "MDS_cmap = LinearSegmentedColormap.from_list('MF_cmap', ['#FFFFFF', MDS_color])\n",
    "MF_cmap = LinearSegmentedColormap.from_list('MF_cmap', ['#FFFFFF', MF_color])\n",
    "any_MN_cmap = LinearSegmentedColormap.from_list('any_MN_cmap', ['#FFFFFF', any_MN_color])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48204b9-b6f5-496c-adcd-fedbcd26124c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b375567-d01c-4c3b-a9f4-8787ca23254e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_train_data(train_data, shuffle=True, random_state=42, ratio=100):\n",
    "    \n",
    "    ## Reduce number of healthy datapoints -- 100 healthy controls per patient\n",
    "    \n",
    "    train_disease = train_data[train_data['disease_status'] == 1]\n",
    "    train_healthy = train_data[train_data['disease_status'] == 0]\n",
    "    n_train_d = len(train_disease['patient_id'].unique())\n",
    "    n_train_h = n_train_d * ratio\n",
    "    healthy_list = train_healthy['patient_id'].unique()\n",
    "    \n",
    "    #random.seed(random_state)\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.shuffle(healthy_list)\n",
    "    \n",
    "    healthy_subset = healthy_list[:n_train_h]\n",
    "    train_healthy_subset = train_healthy[train_healthy['patient_id'].isin(healthy_subset)].reset_index(drop=True)\n",
    "    train_data = pd.concat([train_disease, train_healthy_subset], axis=0)\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ba7e0a-1cd0-4840-a849-a7e6221a453c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '~/mounts/research/husdatalake/disease/scripts/Preleukemia/oona_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0195be4a-4eb4-4e2f-b1ca-9b8f9f532bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = 'MF'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e866dbde-9c00-400a-b3da-a5683d9b4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if disease == 'de_novo_AML':\n",
    "    cmap = AML_cmap\n",
    "if disease == 'MDS':\n",
    "    cmap = MDS_cmap\n",
    "if disease == 'MF':\n",
    "    cmap = MF_cmap\n",
    "if disease == 'any_MN':\n",
    "    cmap = any_MN_cmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c09a91f-4ce5-4bec-9108-38859eea32ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting if patient will get disease during the next X days\n",
    "prediction_horizon = 365 *5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dd250e-e5ae-408b-9648-c557dc882bec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# How many previous datapoints are needed for applying trajectory model\n",
    "min_points=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19c84f7-c45a-4ad7-9bab-b6f4eddb028f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Whether to include hard positives\n",
    "include_hp = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07d6219-13a8-4df9-83e3-6465e70c0736",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimize = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14512659-8784-4760-97fe-eb433536302c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read first stage cox model binary threshold\n",
    "\n",
    "with open('results/basic_model/' + disease + '_threshold_youden.json', 'r') as f:\n",
    "    thresholds = json.load(f)\n",
    "\n",
    "binary_threshold = thresholds['med']\n",
    "print(binary_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0378229-2356-4221-a165-5f78f73efbf9",
   "metadata": {},
   "source": [
    "# 1. Read risk score feature data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3542e2bc-4e24-4403-8f01-f0a8611c6090",
   "metadata": {},
   "outputs": [],
   "source": [
    "if include_hp == True:\n",
    "    features_df = pd.read_csv('trajectory_model/' + disease + '_full_risk_score_deriv_data_with_hp.csv', engine='c', low_memory=False)\n",
    "else:\n",
    "    features_df = pd.read_csv('trajectory_model/' + disease + '_full_risk_score_deriv_data.csv', engine='c', low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc77190",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df['score_gt_thresh'] = features_df['risk_score_now'] >= binary_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebff5f5-640b-47dd-b6bc-58c2652202a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create label based on prediction horizon\n",
    "features_df['label'] = np.where(\n",
    "    (features_df['disease_status'] == 1) & (features_df['time_to_dg'] >= -prediction_horizon),\n",
    "    1,\n",
    "    0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9132a497-40e0-4c5a-85fd-5384a6172eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Disease patients to positive\n",
    "features_df.loc[features_df['label'] == 1, 'time_to_dg'] = features_df.loc[features_df['label'] == 1, 'time_to_dg'].abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa1a4eef-cbe1-4623-ae85-1447f7d2fcbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def4d3ff-fb49-4fb0-acc3-d3176636da69",
   "metadata": {},
   "outputs": [],
   "source": [
    "features_df = features_df[features_df['n_prev']>= min_points]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca5829f7-e0c7-4d87-9f8f-9765e8d360a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77125750-8cab-47f7-b1e2-23c193fe796f",
   "metadata": {},
   "source": [
    "## Read optimized hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf1405-9e97-4ee3-bf26-590244734d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = pd.read_csv('trajectory_model/' + disease + '_hyperparameter_results_cv.csv')\n",
    "max_idx = hyperparams['AUC_mean'].idxmax()\n",
    "params = ast.literal_eval(hyperparams['params'].loc[max_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc5bd05-7514-4269-8f45-0628cc80df3f",
   "metadata": {},
   "source": [
    "# 2. Optimize threshold with 10-fold cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be346c1c-6081-41c0-9deb-0443f0795353",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrounds=1000\n",
    "early_stop=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f711899e-75a8-4898-8542-1eee62279ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2eea1b37-65b9-4966-ab41-b51fa59f4377",
   "metadata": {},
   "outputs": [],
   "source": [
    "youden_thresholds=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ff340e-2775-4845-8192-0d3395b2cb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45469d4e-8f13-4e53-a9b3-1535ea86daf2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(cv):\n",
    "\n",
    "    print('\\n\\tCV loop no: ', i+1)\n",
    "\n",
    "    rs = random.randint(1, 1000)\n",
    "    print(rs)\n",
    "    rs=42\n",
    "    \n",
    "    train, val = train_val_split(features_df,random_state=rs, train_perc=0.85)\n",
    "\n",
    "    # Sanity check - is any of test indices in validation or training sets\n",
    "    print('\\nSanity check: Is there any validaion data in train set')\n",
    "    train_ht = list(train['patient_id'].unique())\n",
    "    validation_ht = list(val['patient_id'].unique())\n",
    "    #test_ht = list(test_data['henkilotunnus'].unique())\n",
    "    val_in_train = np.intersect1d(validation_ht, train_ht).size > 0\n",
    "    print(val_in_train)\n",
    "    \n",
    "    # Train classifier on rows with enough prior data\n",
    "    train = train[train['n_prev'] >= min_points].copy()\n",
    "    \n",
    "    # <ratio> controls per 1 patient\n",
    "    print('N train data rows before reduction: ', len(train))\n",
    "    train = reduce_train_data(train, ratio=ratio)\n",
    "    print('N train data rows after reduction: ', len(train))\n",
    "    \n",
    "    X_train = train.drop(columns=['patient_id', 'disease_status', 'time_to_dg', 'label', 'score_gt_thresh'])\n",
    "    y_train = train['time_to_dg']\n",
    "    X_val = val.drop(columns=['patient_id', 'disease_status', 'time_to_dg', 'label', 'score_gt_thresh'])\n",
    "    y_val = val['time_to_dg']\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    \n",
    "    # Create DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(X_train_scaled, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val_scaled, label=y_val)\n",
    "    \n",
    "    # Use validation set to watch performance\n",
    "    watchlist = [(dtrain,'train'), (dval,'eval')]\n",
    "    \n",
    "    # Store validation results\n",
    "    evals_results = {}\n",
    "    \n",
    "    # Train the model\n",
    "    print(f'\\nTraining the model with parameters: ')\n",
    "    print(params)\n",
    "    \n",
    "    xgb_model = xgb.train(params, dtrain, num_boost_round=nrounds, early_stopping_rounds=early_stop, evals=watchlist, evals_result=evals_results, verbose_eval=50)\n",
    "    \n",
    "    # Predict risk scores\n",
    "    risk_scores_train = xgb_model.predict(dtrain)\n",
    "    risk_scores_val = xgb_model.predict(dval)\n",
    "    \n",
    "    # Add risk scores to the dataframe\n",
    "    train['risk_score'] = risk_scores_train\n",
    "    val['risk_score'] = risk_scores_val\n",
    "    \n",
    "    # Calculate C-index for validation set\n",
    "    # Negative times to positive for getting c-index\n",
    "    val['time_to_dg'] = val['time_to_dg'].apply(lambda x: -x if x < 0 else x)\n",
    "\n",
    "    # AUC-ROC\n",
    "    fpr, tpr, thresholds = roc_curve(val['label'], val['risk_score'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "    # Plotting the ROC curve\n",
    "    fig = plt.figure(figsize=(5,5))\n",
    "    plt.plot(fpr, tpr, lw=3, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    #plt.scatter(optimal_fpr, optimal_tpr, color='r', zorder=5, label='Youden Index', marker='o',s=100)\n",
    "    #plt.scatter(f1_optimal_fpr, f1_optimal_tpr, color='b', zorder=5, label='F1 Index', marker='o',s=100)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.3)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=15)\n",
    "    plt.ylabel('True Positive Rate', fontsize=15)\n",
    "    plt.title(f'Validation data', fontsize=15)\n",
    "    plt.xticks(fontsize=15, rotation=0)\n",
    "    plt.yticks(fontsize=15, rotation=0)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    sns.despine(fig=fig, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "    # Calculate precision and recall\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(val['label'], val['risk_score'])\n",
    "    average_precision = average_precision_score(val['label'], val['risk_score'])\n",
    "\n",
    "    # Calculate youden index\n",
    "    youden_index = tpr - fpr\n",
    "    optimal_threshold_index = np.argmax(youden_index)\n",
    "    best_threshold = thresholds[optimal_threshold_index]\n",
    "    optimal_fpr = fpr[optimal_threshold_index]\n",
    "    optimal_tpr = tpr[optimal_threshold_index]\n",
    "    youden_thresholds.append(best_threshold)\n",
    "\n",
    "    print(f\"Youden index for for validation data: {best_threshold}\")\n",
    "    \n",
    "    \n",
    "    val['simple_label'] = val['score_gt_thresh'].astype(int)\n",
    "    val['predicted_label'] = (val['risk_score'] >= best_threshold).astype(int)\n",
    "    \n",
    "    cm1 = sklearn.metrics.confusion_matrix(val['label'], val['predicted_label'])\n",
    "    \n",
    "    # Consufion matrix color represents % of predictions within the two classes\n",
    "    color_cm1 = np.array([[cm1[0][0] / (cm1[0][0] + cm1[0][1]), cm1[0][1] / (cm1[0][0] + cm1[0][1])],\n",
    "                              [cm1[1][0] / (cm1[1][0] + cm1[1][1]), cm1[1][1] / (cm1[1][0] + cm1[1][1])]])\n",
    "    \n",
    "    fig = plt.figure(figsize=(5,5), dpi=100)\n",
    "    group_counts = ['{0:0.0f}'.format(value) for value in cm1.flatten()]\n",
    "    flat = cm1.flatten()\n",
    "    rows = cm1.sum(1)\n",
    "    values = [flat[0] / rows[0], flat[1] / rows[0], flat[2] / rows[1], flat[3] / rows[1]]\n",
    "    group_percentages = ['{0:.1%}'.format(value) for value in values]\n",
    "    labels = [f'{v1}\\n\\n{v2}' for v1, v2 in zip(group_counts,group_percentages)]\n",
    "    labels = np.asarray(labels).reshape(2,2)\n",
    "    sns.heatmap(color_cm1, annot=labels, annot_kws={'size': 15}, fmt='',linewidths=3, cmap=cmap, cbar=False)#.set(ylabel='True label')\n",
    "    plt.xticks(fontsize=fs, rotation=0)\n",
    "    plt.yticks(fontsize=fs, rotation=0)\n",
    "    plt.show()\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240ad112-7daa-48c4-ac2d-1a432ae9fc01",
   "metadata": {},
   "outputs": [],
   "source": [
    "youden_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e7e818-ac4c-4de0-9fce-bd4ff4d591d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    avg_binary_threshold = np.mean(youden_thresholds)\n",
    "    med_binary_threshold = np.median(youden_thresholds)\n",
    "    print('Optimized avg binary threshold:', avg_binary_threshold)\n",
    "    print('Optimized med binary threshold:', med_binary_threshold)\n",
    "    \n",
    "    tr = {'avg' : float(avg_binary_threshold), 'med' : float(med_binary_threshold)}\n",
    "    with open('trajectory_model/' +  disease + '_threshold.json', 'w') as f:\n",
    "        json.dump(tr, f, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b2033-0b5c-40a5-be8d-34491e1ee7dd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
