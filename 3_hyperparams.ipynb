{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3bc4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "import sklearn\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "#from lifelines.utils import concordance_index\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sksurv.metrics import cumulative_dynamic_auc, concordance_index_censored\n",
    "import ast\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583bd82b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(deriv_data, shuffle=True, random_state=42):\n",
    "    # Divide patients to train / validation / groups\n",
    "    \n",
    "    random.seed(random_state)\n",
    "    # Divide patients to train / validation / groups\n",
    "    \n",
    "    patient_list = deriv_data['henkilotunnus'].unique()\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.shuffle(patient_list)\n",
    "    \n",
    "    # Calculate the number of items in each sublist\n",
    "    total_items = len(patient_list)\n",
    "    train_size = int(total_items * 0.85)\n",
    "    val_size = total_items - train_size  # To ensure all items are included\n",
    "\n",
    "    # Divide the list into sublists\n",
    "    train_list = patient_list[:train_size]\n",
    "    val_list = patient_list[train_size:]\n",
    "    \n",
    "    train_data = deriv_data[deriv_data['henkilotunnus'].isin(train_list)].reset_index(drop=True)\n",
    "    val_data = deriv_data[deriv_data['henkilotunnus'].isin(val_list)].reset_index(drop=True)\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c71c6ea-109b-4ff4-a79a-644f69442792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_train_data(train_data, shuffle=True, random_state=42):\n",
    "    \n",
    "    ## Reduce number of healthy datapoints -- 100 healthy controls per patient\n",
    "    \n",
    "    train_disease = train_data[train_data['disease'] == 1]\n",
    "    train_healthy = train_data[train_data['disease'] == 0]\n",
    "    n_train_d = len(train_disease['henkilotunnus'].unique())\n",
    "    n_train_h = n_train_d * 100\n",
    "    healthy_list = train_healthy['henkilotunnus'].unique()\n",
    "    \n",
    "    random.seed(random_state)\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.shuffle(healthy_list)\n",
    "    \n",
    "    healthy_subset = healthy_list[:n_train_h]\n",
    "    train_healthy_subset = train_healthy[train_healthy['henkilotunnus'].isin(healthy_subset)].reset_index(drop=True)\n",
    "    train_data = pd.concat([train_disease, train_healthy_subset], axis=0)\n",
    "\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db28cb3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '~/mounts/research/husdatalake/disease/scripts/Preleukemia/oona_new'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed836071",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = 'any_MN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea215c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_data = pd.read_csv(my_path + '/data/modelling/' + disease + '_derivation_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d23559f",
   "metadata": {},
   "source": [
    "## Hyperparameter optimization with cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0977de1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0763cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparametes to evaluate\n",
    "\n",
    "param_grid = {\n",
    "    'objective' : ['survival:cox'],\n",
    "    'eval_metric' : ['cox-nloglik'],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'eta': [0.01, 0.05, 0.1],\n",
    "    'min_child_weight': [30, 50, 100],\n",
    "    'subsample': [0.5,0.8],\n",
    "    'colsample_bytree': [0.7],\n",
    "    'lambda': [0.5,10],\n",
    "    'alpha': [0.5],\n",
    "    'tree_method': ['hist']\n",
    "}\n",
    "\n",
    "nrounds = 1000\n",
    "early_stop = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a5ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(parameters_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9661abad",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c71ad563",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame(columns=['params', 'c_index_mean', 'c_index_std', 'AUC_mean', 'AUC_std', 'AUCPR_mean', 'AUCPR_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb81d4a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for params in parameters_grid:\n",
    "    \n",
    "    print(f'\\nTRAINING MODEL WITH PARAMETERS:\\n {params} \\n{cv}-FOLD CROSS VALIDATION')\n",
    "    \n",
    "    c_indices = []\n",
    "    AUCs = []\n",
    "    AUCPRs = []\n",
    "\n",
    "    for i in range(cv):\n",
    "\n",
    "        print('\\n\\tCV loop no: ', i+1)\n",
    "\n",
    "        train_data, validation_data = train_val_split(deriv_data)\n",
    "\n",
    "        # 100 controls per 1 patient\n",
    "        print('N train data rows before reduction: ', len(train_data))\n",
    "        train_data = reduce_train_data(train_data)\n",
    "        print('N train data rows after reduction: ', len(train_data))\n",
    "    \n",
    "        # Drop hard positive rows from validation data\n",
    "        validation_data = validation_data[validation_data['hp'] != 1]\n",
    "    \n",
    "        ## DELETE hp COLUMN FROM TRAIN / VAL\n",
    "        train_data = train_data.drop(columns=['hp'])\n",
    "        validation_data = validation_data.drop(columns=['hp'])\n",
    "\n",
    "        # Separate features and target variables\n",
    "        x_train = train_data.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "        y_train = train_data['time_to_dg']\n",
    "\n",
    "        x_val = validation_data.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "        y_val = validation_data['time_to_dg']\n",
    "\n",
    "        # Create DMatrix for XGBoost\n",
    "        dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "        dval = xgb.DMatrix(x_val, label=y_val)\n",
    "\n",
    "        # Use validation set to watch performance\n",
    "        watchlist = [(dtrain,'train'), (dval,'eval')]\n",
    "\n",
    "        # Store validation results\n",
    "        evals_results = {}\n",
    "\n",
    "        # Train the model\n",
    "        xgb_model = xgb.train(params, dtrain, num_boost_round=nrounds, early_stopping_rounds=early_stop, evals=watchlist, evals_result=evals_results, verbose_eval=50)\n",
    "\n",
    "        # Predict risk scores\n",
    "        risk_scores_train = xgb_model.predict(dtrain)\n",
    "        risk_scores_val = xgb_model.predict(dval)\n",
    "\n",
    "        # Add risk scores to the dataframe\n",
    "        train_data['risk_score'] = risk_scores_train\n",
    "        validation_data['risk_score'] = risk_scores_val\n",
    "\n",
    "        # Calculate C-index for validation set\n",
    "        # Negative times to positive for getting c-index\n",
    "        validation_data['time_to_dg'] = validation_data['time_to_dg'].apply(lambda x: -x if x < 0 else x)\n",
    "        \n",
    "        try:\n",
    "            c_index = concordance_index_censored(event_indicator=validation_data['disease'].replace({0 : False, 1 : True}), event_time=validation_data['time_to_dg'], estimate=validation_data['risk_score'])[0]\n",
    "    \n",
    "            # ROC-AUC\n",
    "            fpr, tpr, thresholds = roc_curve(validation_data['disease'], validation_data['risk_score'])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "\n",
    "            # Calculate precision and recall\n",
    "            precision, recall, pr_thresholds = precision_recall_curve(validation_data['disease'], validation_data['risk_score'])\n",
    "            AUCPR = average_precision_score(validation_data['disease'], validation_data['risk_score'])\n",
    "\n",
    "            c_indices.append(c_index)\n",
    "            AUCs.append(roc_auc)\n",
    "            AUCPRs.append(AUCPR)\n",
    "        \n",
    "        except:\n",
    "            print('Something went wrong with model training with current parameters.')\n",
    " \n",
    "    result_df.loc[len(result_df.index)] = [params, np.mean(c_indices), np.std(c_indices), np.mean(AUCs), np.std(AUCs), np.mean(AUCPRs), np.std(AUCPRs)]\n",
    "    \n",
    "    # Save results at each iteration\n",
    "    result_df.to_csv(my_path + '/optimization/hyperparams/' + disease + '_hyperparameter_results_cv_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70fe3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add8f6d7-a7bc-4f4f-a043-a1e4a37b3740",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUCPRs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9ea077-7fa8-4549-8964-d9469a950c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336e9b9f-ebc5-4337-b771-0efa76a227aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae04b4d-b8f8-47d7-81d8-21315923d90c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e31cfd-1f34-4b17-9e9e-43bcd663cfb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c047875b-cd13-4242-949d-1233c272513e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
