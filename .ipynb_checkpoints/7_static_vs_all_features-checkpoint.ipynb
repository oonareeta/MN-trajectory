{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1854541f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "import sklearn\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sksurv.metrics import cumulative_dynamic_auc, concordance_index_censored\n",
    "import ast\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51338a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(deriv_data, shuffle=True, random_state=42):\n",
    "    # Divide patients to train / validation / groups\n",
    "    \n",
    "    random.seed(random_state)\n",
    "    # Divide patients to train / validation / groups\n",
    "    \n",
    "    patient_list = deriv_data['henkilotunnus'].unique()\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.shuffle(patient_list)\n",
    "    \n",
    "    # Calculate the number of items in each sublist\n",
    "    total_items = len(patient_list)\n",
    "    train_size = int(total_items * 0.75)\n",
    "    val_size = total_items - train_size  # To ensure all items are included\n",
    "\n",
    "    # Divide the list into sublists\n",
    "    train_list = patient_list[:train_size]\n",
    "    val_list = patient_list[train_size:]\n",
    "    \n",
    "    train_data = deriv_data[deriv_data['henkilotunnus'].isin(train_list)].reset_index(drop=True)\n",
    "    val_data = deriv_data[deriv_data['henkilotunnus'].isin(val_list)].reset_index(drop=True)\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fadde161",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '~/mounts/research/husdatalake/disease/scripts/Preleukemia/oona_git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78ad816-f1c1-438e-abb3-657a043bccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs=15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006caed4-eb08-4331-a3d6-88b19f5da22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = 'MDS'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab4260-d9d0-4bcc-9e47-a88dd59f5dbf",
   "metadata": {},
   "source": [
    "### Static vs all features cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7bf275-bf15-444c-a437-7aaa48caf56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 10\n",
    "nrounds = 1000\n",
    "early_stop = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2a5028-e1c9-4bd0-ae3d-917ac2d210f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "for metric in ['ROC','PR']:\n",
    "    \n",
    "    static_AUCs = []\n",
    "    static_AUCPRs = []\n",
    "    all_AUCs = []\n",
    "    all_AUCPRs = []\n",
    "    \n",
    "    print('\\n', disease)\n",
    "    \n",
    "    deriv_data = pd.read_csv(my_path + '/data/modelling/' + disease + '_derivation_data.csv')\n",
    "    \n",
    "    # Drop e_retic columns\n",
    "    deriv_data = deriv_data.loc[:, ~deriv_data.columns.str.startswith('e_retic')]\n",
    "    all_features = list(deriv_data.columns)\n",
    "    basic_features = ['henkilotunnus', 'time_to_dg', 'disease','sukupuoli_selite', 'age', 'rows_in_last_month']\n",
    "    \n",
    "    # Read hyperparameters    \n",
    "    hyperparams = pd.read_csv(my_path + '/optimization/hyperparams/' + disease + '_hyperparameter_results_cv.csv', index_col=0)\n",
    "    max_idx = hyperparams['AUCPR_mean'].idxmax()  #f1_score_mean\n",
    "    params = ast.literal_eval(hyperparams['params'].loc[max_idx])\n",
    "    \n",
    "    include = []\n",
    "    for feat in all_features:\n",
    "        if ( 'norm' in feat ):\n",
    "            include.append(feat)\n",
    "                \n",
    "    basic_features.extend(include)\n",
    "    \n",
    "    feature_pool = [x for x in all_features if x not in basic_features]\n",
    "    \n",
    "    for feature_type in ['static', '']:\n",
    "    \n",
    "        include = []\n",
    "        for feat in feature_pool:\n",
    "            if feature_type in feat:\n",
    "                include.append(feat)\n",
    "    \n",
    "        deriv_iter = deriv_data[basic_features + include]\n",
    "    \n",
    "        if feature_type == '':\n",
    "            feature_type = 'all'\n",
    "            \n",
    "        print('\\nTraining model with features : ', feature_type)\n",
    "    \n",
    "        print('')\n",
    "        print(feature_type)\n",
    "        print(f'Using {len(list(deriv_iter.columns))} features')\n",
    "        print('')\n",
    "    \n",
    "        fig = plt.figure(figsize=(6,6))\n",
    "        \n",
    "        # Do cross validation\n",
    "        for i in range(cv):\n",
    "        \n",
    "            print('\\n\\tCV loop no: ', i+1)\n",
    "    \n",
    "            # Train model & evaluate\n",
    "            train_data, validation_data = train_val_split(deriv_iter, shuffle=True, random_state=i+1)\n",
    "\n",
    "            # Separate features and target variables\n",
    "            x_train = train_data.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "            y_train = train_data['time_to_dg']\n",
    "    \n",
    "            x_val = validation_data.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "            y_val = validation_data['time_to_dg']\n",
    "    \n",
    "            # Create DMatrix for XGBoost\n",
    "            dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "            dval = xgb.DMatrix(x_val, label=y_val)\n",
    "    \n",
    "            # Use validation set to watch performance\n",
    "            watchlist = [(dtrain,'train'), (dval,'eval')]\n",
    "    \n",
    "            # Store validation results\n",
    "            evals_results = {}\n",
    "    \n",
    "            # Train the model\n",
    "            print(f'\\nTraining the model with parameters: ')\n",
    "            print(params)\n",
    "    \n",
    "            xgb_model = xgb.train(params, dtrain, num_boost_round=nrounds, early_stopping_rounds=early_stop, evals=watchlist, evals_result=evals_results, verbose_eval=50)\n",
    "    \n",
    "            # Predict risk scores\n",
    "            risk_scores_train = xgb_model.predict(dtrain)\n",
    "            risk_scores_val = xgb_model.predict(dval)\n",
    "    \n",
    "            # Add risk scores to the dataframe\n",
    "            train_data['risk_score'] = risk_scores_train\n",
    "            validation_data['risk_score'] = risk_scores_val\n",
    "    \n",
    "            # Negative times to positive for getting c-index\n",
    "            validation_data['time_to_dg'] = validation_data['time_to_dg'].apply(lambda x: -x if x < 0 else x)\n",
    "            #c_index = concordance_index_censored(event_indicator=validation_data['disease'].replace({0 : False, 1 : True}), event_time=validation_data['time_to_dg'], estimate=validation_data['risk_score'])[0]\n",
    "    \n",
    "            # Calculate C-index for validation set\n",
    "            #c_index = concordance_index(validation_data['time_to_dg'], -validation_data['risk_score'], validation_data['disease'])\n",
    "            fpr, tpr, thresholds = roc_curve(validation_data['disease'], validation_data['risk_score'])\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "    \n",
    "            # Calculate precision and recall\n",
    "            precision, recall, pr_thresholds = precision_recall_curve(validation_data['disease'], validation_data['risk_score'])\n",
    "            average_precision = average_precision_score(validation_data['disease'], validation_data['risk_score'])\n",
    "    \n",
    "            if feature_type == 'static':\n",
    "                color = \"#052f82\"\n",
    "            else:\n",
    "                color = \"#820e05\"\n",
    "                \n",
    "            if metric == 'ROC':\n",
    "                plt.plot(fpr, tpr, lw=2, label=f'{feature_type} (AUC = {round(roc_auc,2)})', color=color)\n",
    "                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.2)\n",
    "                plt.xlim([0.0, 1.0])\n",
    "                plt.ylim([0.0, 1.05])\n",
    "                plt.ylabel('Sensitivity', fontsize=fs)\n",
    "                plt.xlabel('1 - Specificity',fontsize=fs)\n",
    "            \n",
    "            else:\n",
    "                plt.step(recall, precision, where='post', label=f'{feature_type} (AUPRC = {round(average_precision,2)})', color=color)\n",
    "                plt.xlabel('Recall', fontsize=fs)\n",
    "                plt.ylabel('Precision', fontsize=fs)\n",
    "    \n",
    "            if feature_type == 'all':\n",
    "                all_AUCs.append(roc_auc)\n",
    "                all_AUCPRs.append(average_precision)\n",
    "            else:\n",
    "                static_AUCs.append(roc_auc)\n",
    "                static_AUCPRs.append(average_precision)\n",
    "    \n",
    "        if feature_type == 'all':\n",
    "            mean_AUC = np.mean(all_AUCs)\n",
    "            mean_AUCPR = np.mean(all_AUCPRs)\n",
    "        else:\n",
    "            mean_AUC = np.mean(static_AUCs)\n",
    "            mean_AUCPR = np.mean(static_AUCPRs)\n",
    "        \n",
    "        plt.xticks(fontsize=fs, rotation=0)\n",
    "        plt.yticks(fontsize=fs, rotation=0)\n",
    "        \n",
    "        if metric == 'ROC':\n",
    "            plt.title(f'{feature_type} features mean AUC= {mean_AUC}', fontsize=fs)\n",
    "        else:\n",
    "            plt.title(f'{feature_type} features mean AUPRC= {mean_AUCPR}', fontsize=fs)\n",
    "        #plt.legend(loc='best')\n",
    "        sns.despine(fig=fig, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False)\n",
    "        plt.show()\n",
    "    \n",
    "        if metric == 'ROC':\n",
    "            fig.savefig('results/final_model/plots/static_vs_all_features/cross_validation/' + disease + '_roc_' + feature_type + '.png')\n",
    "        else:\n",
    "            fig.savefig('results/final_model/plots/static_vs_all_features/cross_validation/' + disease + '_pr_' + feature_type + '.png')\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5f345a2-b9b9-435b-ac3e-1ed320a2b7b9",
   "metadata": {},
   "source": [
    "## Create vector across 10 cv loops for delong test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d592f7-ef1c-4ec8-bd0b-669d92aa3949",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_vector = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2815605f-da9b-4d38-be65-534e062cc1fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8205c563-24e3-4b68-9e55-9a541beb2cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n', disease)\n",
    "\n",
    "cv_ytrue_static = []\n",
    "cv_ytrue_all = []\n",
    "cv_ypred_static = []\n",
    "cv_ypred_all = []\n",
    "\n",
    "deriv_data = pd.read_csv(my_path + '/data/modelling/' + disease + '_derivation_data.csv')\n",
    "\n",
    "# Drop e_retic columns\n",
    "deriv_data = deriv_data.loc[:, ~deriv_data.columns.str.startswith('e_retic')]\n",
    "all_features = list(deriv_data.columns)\n",
    "basic_features = ['henkilotunnus', 'time_to_dg', 'disease','sukupuoli_selite', 'age', 'rows_in_last_month']\n",
    "\n",
    "# Read hyperparameters    \n",
    "hyperparams = pd.read_csv(my_path + '/optimization/hyperparams/' + disease + '_hyperparameter_results_cv.csv', index_col=0)\n",
    "max_idx = hyperparams['AUCPR_mean'].idxmax()  #f1_score_mean\n",
    "params = ast.literal_eval(hyperparams['params'].loc[max_idx])\n",
    "\n",
    "include = []\n",
    "for feat in all_features:\n",
    "        if ( 'norm' in feat ):\n",
    "            include.append(feat)\n",
    "            \n",
    "basic_features.extend(include)\n",
    "\n",
    "feature_pool = [x for x in all_features if x not in basic_features]\n",
    "\n",
    "deriv_static = deriv_data[basic_features]\n",
    "\n",
    "include = []\n",
    "for feat in feature_pool:\n",
    "    if '' in feat:\n",
    "        include.append(feat)\n",
    "\n",
    "deriv_all = deriv_data[basic_features + include]\n",
    "\n",
    "\n",
    "# Do cross validation\n",
    "for i in range(cv):\n",
    "\n",
    "    print('\\n\\tCV loop no: ', i+1)\n",
    "\n",
    "    print('\\nTraining model with static features')\n",
    "    \n",
    "    # Train model & evaluate\n",
    "    train_data_static, validation_data_static = train_val_split(deriv_static, shuffle=True, random_state=i+1)\n",
    "    print(len(validation_data_static))\n",
    "    \n",
    "    # Separate features and target variables\n",
    "    x_train = train_data_static.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "    y_train = train_data_static['time_to_dg']\n",
    "    \n",
    "    x_val = validation_data_static.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "    y_val = validation_data_static['time_to_dg']\n",
    "    \n",
    "    # Create DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    dval = xgb.DMatrix(x_val, label=y_val)\n",
    "    \n",
    "    # Use validation set to watch performance\n",
    "    watchlist = [(dtrain,'train'), (dval,'eval')]\n",
    "    \n",
    "    # Store validation results\n",
    "    evals_results = {}\n",
    "    \n",
    "    # Train the model\n",
    "    print(f'\\nTraining the model with parameters: ')\n",
    "    print(params)\n",
    "    \n",
    "    xgb_model_static = xgb.train(params, dtrain, num_boost_round=nrounds, early_stopping_rounds=early_stop, evals=watchlist, evals_result=evals_results, verbose_eval=50)\n",
    "    \n",
    "    y_true_static = validation_data_static['disease']\n",
    "    y_pred_static = xgb_model_static.predict(dval)\n",
    "    \n",
    "    print('\\nTraining model with all features')\n",
    "    \n",
    "    # Train model & evaluate\n",
    "    train_data_all, validation_data_all = train_val_split(deriv_all, shuffle=True, random_state=i+1)\n",
    "    print(len(validation_data_all))\n",
    "    \n",
    "    # Separate features and target variables\n",
    "    x_train = train_data_all.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "    y_train = train_data_all['time_to_dg']\n",
    "    \n",
    "    x_val = validation_data_all.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "    y_val = validation_data_all['time_to_dg']\n",
    "    \n",
    "    # Create DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    dval = xgb.DMatrix(x_val, label=y_val)\n",
    "    \n",
    "    # Use validation set to watch performance\n",
    "    watchlist = [(dtrain,'train'), (dval,'eval')]\n",
    "    \n",
    "    # Store validation results\n",
    "    evals_results = {}\n",
    "    \n",
    "    # Train the model\n",
    "    print(f'\\nTraining the model with parameters: ')\n",
    "    print(params)\n",
    "    \n",
    "    xgb_model_all = xgb.train(params, dtrain, num_boost_round=nrounds, early_stopping_rounds=early_stop, evals=watchlist, evals_result=evals_results, verbose_eval=50)\n",
    "    \n",
    "    y_true_all = validation_data_all['disease']\n",
    "    y_pred_all = xgb_model_all.predict(dval)\n",
    "    \n",
    "    cv_ytrue_static.extend(y_true_static.to_list())\n",
    "    cv_ytrue_all.extend(y_true_all.to_list())\n",
    "    cv_ypred_static.extend(list(y_pred_static))\n",
    "    cv_ypred_all.extend(list(y_pred_all))\n",
    "    print(len(cv_ytrue_static))\n",
    "\n",
    "\n",
    "# After the cv loop\n",
    "cv_vector['y_true'] = cv_ytrue_static\n",
    "cv_vector['y_pred_static'] = cv_ypred_static\n",
    "cv_vector['y_pred_all'] = cv_ypred_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d8247d-69be-4e4f-b456-bc869d068591",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3ec7a7-8d24-4f52-a54d-38d44e6f48b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_vector.to_csv('results/final_model/plots/static_vs_all_features/' + disease + '_static_vs_all_delong_cv_vector.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b3e463-616b-43d4-8b5e-1569c524aaf5",
   "metadata": {},
   "source": [
    "## READ CV VECTOR HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f49241-cb03-4636-9b42-19c1d203695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_vector = pd.read_csv('results/final_model/plots/static_vs_all_features/' + disease + '_static_vs_all_delong_cv_vector.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6995d1ea-60f7-4936-a5f1-f021c6ae9f45",
   "metadata": {},
   "source": [
    "## Plot static vs all roc & pr curves across 10-cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fbe73d7-55be-420b-9112-7ee8d076f15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_fpr, static_tpr, static_thresholds = roc_curve(cv_vector['y_true'], cv_vector['y_pred_static'])\n",
    "static_roc_auc = auc(static_fpr, static_tpr)\n",
    "\n",
    "\n",
    "all_fpr, all_tpr, all_thresholds = roc_curve(cv_vector['y_true'], cv_vector['y_pred_all'])\n",
    "all_roc_auc = auc(all_fpr, all_tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b2e318f-eb48-4821-8a94-d11f56887ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if disease == 'de_novo_AML':\n",
    "    name = 'De novo AML'\n",
    "\n",
    "if disease == 'primary_MF':\n",
    "    name = 'Primary MF'\n",
    "\n",
    "if disease == 'any_MN':\n",
    "    name = 'Any MN'\n",
    "\n",
    "if disease == 'MDS':\n",
    "    name = 'MDS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ba766a1-0c18-4dbe-895a-d25a4993ca24",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.2)\n",
    "plt.plot(static_fpr, static_tpr, lw=3, label='Static (AUROC = %0.2f)' % static_roc_auc, color = \"#052f82\") #, color=any_MN_color)\n",
    "plt.plot(all_fpr, all_tpr, lw=3, label='All (AUROC = %0.2f)' % all_roc_auc, color = \"#820e05\") #label='Any MN (AUROC = %0.2f)' % any_MN_roc_auc, color=any_MN_color)\n",
    "sns.despine(fig=fig, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False)\n",
    "plt.legend()\n",
    "plt.ylabel('Sensitivity', fontsize=fs)\n",
    "plt.xlabel('1 - Specificity',fontsize=fs)\n",
    "plt.xticks(fontsize=fs, rotation=0)\n",
    "plt.yticks(fontsize=fs, rotation=0)\n",
    "plt.title(name, loc='left', fontsize=fs)\n",
    "fig.savefig('results/final_model/plots/static_vs_all_features/' + disease + '_static_vs_all_roc.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b17c4a-f473-4100-ad9b-8beac8227354",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_precision, static_recall, static_thresholds = precision_recall_curve(cv_vector['y_true'], cv_vector['y_pred_static'])\n",
    "static_average_precision = average_precision_score(cv_vector['y_true'], cv_vector['y_pred_static'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e913269-9c17-48f4-889b-ace8ff30ddbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_precision, all_recall, all_thresholds = precision_recall_curve(cv_vector['y_true'], cv_vector['y_pred_all'])\n",
    "all_average_precision = average_precision_score(cv_vector['y_true'], cv_vector['y_pred_all'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4f6ce-c16f-4e29-bab5-0830fdd5f603",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "plt.step(static_recall, static_precision, where='post', label=f'Static (AUPRC = {static_average_precision:.2f})',lw=3, color = \"#052f82\")\n",
    "plt.step(all_recall, all_precision, where='post', label=f'All (AUPRC = {all_average_precision:.2f})',lw=3, color = \"#820e05\")\n",
    "sns.despine(fig=fig, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False)\n",
    "plt.xlabel('Recall', fontsize=fs)\n",
    "plt.ylabel('Precision', fontsize=fs)\n",
    "plt.xticks(fontsize=fs, rotation=0)\n",
    "plt.yticks(fontsize=fs, rotation=0)\n",
    "plt.legend()\n",
    "plt.title(name, loc='left', fontsize=fs)\n",
    "fig.savefig('results/final_model/plots/static_vs_all_features/' + disease + '_static_vs_all_pr.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b77224-f86e-4175-88e8-c50ab2312dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
