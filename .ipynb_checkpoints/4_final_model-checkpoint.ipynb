{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc2c8e53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import math\n",
    "import warnings\n",
    "import sklearn\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "from sklearn.metrics import roc_curve, auc, f1_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sksurv.metrics import cumulative_dynamic_auc, concordance_index_censored\n",
    "import ast\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2526e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(deriv_data, shuffle=True, random_state=42):\n",
    "    # Divide patients to train / validation / groups\n",
    "    \n",
    "    random.seed(random_state)\n",
    "    # Divide patients to train / validation / groups\n",
    "    \n",
    "    patient_list = deriv_data['henkilotunnus'].unique()\n",
    "    \n",
    "    if shuffle == True:\n",
    "        random.shuffle(patient_list)\n",
    "    \n",
    "    # Calculate the number of items in each sublist\n",
    "    total_items = len(patient_list)\n",
    "    train_size = int(total_items * 0.85)\n",
    "    val_size = total_items - train_size  # To ensure all items are included\n",
    "\n",
    "    # Divide the list into sublists\n",
    "    train_list = patient_list[:train_size]\n",
    "    val_list = patient_list[train_size:]\n",
    "    \n",
    "    train_data = deriv_data[deriv_data['henkilotunnus'].isin(train_list)].reset_index(drop=True)\n",
    "    val_data = deriv_data[deriv_data['henkilotunnus'].isin(val_list)].reset_index(drop=True)\n",
    "\n",
    "    return train_data, val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e11310",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_path = '~/mounts/research/husdatalake/disease/scripts/Preleukemia/oona_git'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0ff4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "disease = 'MDS'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b955ddb-7115-452c-99ef-4bc20ec34749",
   "metadata": {},
   "source": [
    "### Read data, features and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb8662b",
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_data = pd.read_csv(my_path + '/data/modelling/' + disease + '_derivation_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0dc51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(my_path + '/data/modelling/' + disease + '_test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992295d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv(my_path + '/optimization/feature_selection/' + disease + '_features.csv')['features'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16db9ec-70f4-4bb2-bd45-9dd9fc6ed8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "deriv_data = deriv_data[['henkilotunnus','time_to_dg','disease'] + features]\n",
    "test_data = test_data[['henkilotunnus','time_to_dg','disease'] + features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f18a2de-6736-437b-9f19-e91148109ff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove columns starting 'e_retic'\n",
    "deriv_data = deriv_data.loc[:, ~deriv_data.columns.str.startswith('e_retic')]\n",
    "test_data = test_data.loc[:, ~test_data.columns.str.startswith('e_retic')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4d922c-a08f-4d09-8a2a-59e7e3062de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Using {len(deriv_data.columns)} features in the final model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2861f3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read hyperparameters\n",
    "hyperparams = pd.read_csv(my_path + '/optimization/hyperparams/' + disease + '_hyperparameter_results_cv.csv', index_col=0)\n",
    "max_idx = hyperparams['AUCPR_mean'].idxmax()\n",
    "params = ast.literal_eval(hyperparams['params'].loc[max_idx])\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df3771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nrounds = 1000\n",
    "early_stop = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dfca4e",
   "metadata": {},
   "source": [
    "### Define binary classification threshold with 10-fold cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a755556",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result_df = pd.DataFrame(index=range(1), columns=['c_index_mean', 'c_index_std', 'AUC_mean', 'AUC_std', 'AUCPR_mean', 'AUCPR_std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f9c024",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2dc21e",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_indices = []\n",
    "AUCs = []\n",
    "AUCPRs = []\n",
    "youden_indices = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d09fd6a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(f'\\nFINDING BINARY CLASSIFICATION THRESHOLD - {cv}-FOLD CROSS VALIDATION')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a0804c-cd34-4d45-9226-b01a3552ce47",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(cv):\n",
    "\n",
    "    print('\\n\\tCV loop no: ', i+1)\n",
    "    \n",
    "    train_data, validation_data = train_val_split(deriv_data, shuffle=True, random_state=None)\n",
    "    \n",
    "    # Check the class ratios\n",
    "    pos_ratio_train = 100 * train_data['disease'].value_counts()[1] / train_data['disease'].value_counts()[0]\n",
    "    pos_ratio_val = 100 * validation_data['disease'].value_counts()[1] / validation_data['disease'].value_counts()[0]\n",
    "    pos_ratio_test = 100 * test_data['disease'].value_counts()[1] / test_data['disease'].value_counts()[0]\n",
    "    print(f'\\n{pos_ratio_train} % of the datapoints in the training set had disease = 1')\n",
    "    print(f'{pos_ratio_val} % of the datapoints in the validation set had disease = 1')\n",
    "\n",
    "    # Sanity check - is any of test indices in validation or training sets\n",
    "    print('\\nSanity check: Is there any validaion data in train set')\n",
    "    train_ht = list(train_data['henkilotunnus'].unique())\n",
    "    validation_ht = list(validation_data['henkilotunnus'].unique())\n",
    "    test_ht = list(test_data['henkilotunnus'].unique())\n",
    "    val_in_train = np.intersect1d(validation_ht, train_ht).size > 0\n",
    "    print(val_in_train)\n",
    "\n",
    "    # Separate features and target variables\n",
    "    x_train = train_data.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "    y_train = train_data['time_to_dg']\n",
    "\n",
    "    x_val = validation_data.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "    y_val = validation_data['time_to_dg']\n",
    "\n",
    "    # Create DMatrix for XGBoost\n",
    "    dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    dval = xgb.DMatrix(x_val, label=y_val)\n",
    "    \n",
    "    # Use validation set to watch performance\n",
    "    watchlist = [(dtrain,'train'), (dval,'eval')]\n",
    "\n",
    "    # Store validation results\n",
    "    evals_results = {}\n",
    "\n",
    "    # Train the model\n",
    "    xgb_model = xgb.train(params, dtrain, num_boost_round=nrounds, early_stopping_rounds=early_stop, evals=watchlist, evals_result=evals_results, verbose_eval=50)\n",
    "\n",
    "    # Predict risk scores\n",
    "    risk_scores_train = xgb_model.predict(dtrain)\n",
    "    risk_scores_val = xgb_model.predict(dval)\n",
    "\n",
    "    # Add risk scores to the dataframe\n",
    "    train_data['risk_score'] = risk_scores_train\n",
    "    validation_data['risk_score'] = risk_scores_val\n",
    "    \n",
    "    # Calculate C-index for validation set\n",
    "    # Negative times to positive for getting c-index\n",
    "    validation_data['time_to_dg'] = validation_data['time_to_dg'].apply(lambda x: -x if x < 0 else x)\n",
    "    c_index = concordance_index_censored(event_indicator=validation_data['disease'].replace({0 : False, 1 : True}), event_time=validation_data['time_to_dg'], estimate=validation_data['risk_score'])[0]\n",
    "    \n",
    "    # AUC-ROC\n",
    "    fpr, tpr, thresholds = roc_curve(validation_data['disease'], validation_data['risk_score'])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    # Compute Youden Index for each threshold\n",
    "    youden_index = tpr - fpr\n",
    "    optimal_threshold_index = np.argmax(youden_index)\n",
    "    optimal_threshold = thresholds[optimal_threshold_index]\n",
    "    optimal_fpr = fpr[optimal_threshold_index]\n",
    "    optimal_tpr = tpr[optimal_threshold_index]\n",
    "    \n",
    "    youden_indices.append(optimal_threshold)\n",
    "\n",
    "    # Plotting the ROC curve\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    plt.plot(fpr, tpr, lw=3, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.scatter(optimal_fpr, optimal_tpr, color='r', zorder=5, label='Youden Index', marker='o',s=100)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', alpha=0.3)\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate', fontsize=15)\n",
    "    plt.ylabel('True Positive Rate', fontsize=15)\n",
    "    plt.title(f'Validation data', fontsize=15)\n",
    "    plt.xticks(fontsize=15, rotation=0)\n",
    "    plt.yticks(fontsize=15, rotation=0)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    sns.despine(fig=fig, ax=None, top=True, right=True, left=False, bottom=False, offset=None, trim=False)\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precision, recall, pr_thresholds = precision_recall_curve(validation_data['disease'], validation_data['risk_score'])\n",
    "    average_precision = average_precision_score(validation_data['disease'], validation_data['risk_score'])\n",
    "    \n",
    "    print(f\"Youden index for for validation data: {optimal_threshold}\")\n",
    "    \n",
    "    c_indices.append(c_index)\n",
    "    AUCs.append(roc_auc)\n",
    "    AUCPRs.append(average_precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c983c162-e339-442b-bcf7-bcbbc25e7dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result_df.loc[0]['c_index_mean'] = np.mean(c_indices)\n",
    "cv_result_df.loc[0]['AUC_mean'] = np.mean(AUCs)\n",
    "cv_result_df.loc[0]['AUCPR_mean'] = np.mean(AUCPRs)\n",
    "\n",
    "cv_result_df.loc[0]['c_index_std'] = np.std(c_indices)\n",
    "cv_result_df.loc[0]['AUC_std'] = np.std(AUCs)\n",
    "cv_result_df.loc[0]['AUCPR_std'] = np.std(AUCPRs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67d8879c-eaa5-4bb0-87c4-2cb0742de88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8d271",
   "metadata": {},
   "source": [
    "### Use average youden index on validation data across 10 cv loops as binary threshold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8245b31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_threshold = np.mean(youden_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404f6643",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455be395-7366-4a15-8b7d-acefeece9d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pre-trained threshold\n",
    "binary_threshold = 0.740484"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0d429b",
   "metadata": {},
   "source": [
    "### Train final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b234b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, validation_data = train_val_split(deriv_data, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37f016c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variables\n",
    "x_train = train_data.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "y_train = train_data['time_to_dg']\n",
    "\n",
    "x_val = validation_data.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "y_val = validation_data['time_to_dg']\n",
    "\n",
    "x_test = test_data.drop(columns=['henkilotunnus', 'disease', 'time_to_dg'])\n",
    "y_test = test_data['time_to_dg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371d30a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save x_train for getting SHAP values\n",
    "x_train.to_csv('results/final_model/SHAP/' + disease + '_x_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bfafc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(deriv_data) + len(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f1927",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(x_train) + len(x_val) + len(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f482e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DMatrix for XGBoost\n",
    "dtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "dval = xgb.DMatrix(x_val, label=y_val)\n",
    "dtest = xgb.DMatrix(x_test, label=y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4bc4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use validation set to watch performance\n",
    "watchlist = [(dtrain,'train'), (dval,'eval')]\n",
    "\n",
    "# Store validation results\n",
    "evals_results = {}\n",
    "\n",
    "# Train the model\n",
    "print(f'\\nTraining the model with parameters: ')\n",
    "print(params)\n",
    "\n",
    "xgb_model = xgb.train(params, dtrain, num_boost_round=nrounds, early_stopping_rounds=early_stop, evals=watchlist, evals_result=evals_results, verbose_eval=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea9e32d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training and validation losses\n",
    "tr_loss = list(evals_results['train'].values())[0]\n",
    "val_loss = list(evals_results['eval'].values())[0]\n",
    "plt.plot(range(len(tr_loss)), tr_loss, label='Training loss')\n",
    "plt.plot(range(len(tr_loss)), val_loss, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb67fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_model.save_model('results/final_model/' + disease + '_final_model.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e54869e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict risk scores\n",
    "risk_scores_train = xgb_model.predict(dtrain)\n",
    "risk_scores_val = xgb_model.predict(dval)\n",
    "risk_scores_test = xgb_model.predict(dtest)\n",
    "\n",
    "# Add risk scores to the dataframe\n",
    "train_data['risk_score'] = risk_scores_train\n",
    "validation_data['risk_score'] = risk_scores_val\n",
    "test_data['risk_score'] = risk_scores_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ff1a54-1689-4d76-8871-de7cafa432e8",
   "metadata": {},
   "source": [
    "### Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "688c1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['time_to_dg'] = test_data['time_to_dg'].apply(lambda x: -x if x < 0 else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b356529-5151-402c-9688-fb4ee0364dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert risk scores to binary predictions using the optimal threshold\n",
    "predicted_labels = (test_data['risk_score'] >= binary_threshold).astype(int)\n",
    "test_data['predicted_disease'] = predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35231ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset with final model predictions for plotting\n",
    "test_data.to_csv(my_path + '/results/final_model/' + disease + '_test_data_with_final_model_predictions.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
